{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gk8znvyMxzz"
   },
   "source": [
    "# Fine tuning Methods:\n",
    "### Additonal Trianing :\n",
    "- training a base model with an additional dataset.\n",
    "### Dreamboot :\n",
    "- developed by google ,a  technique for injecting custom subjects into model, DUe to its architecture , it is possible to achieve great results using only 3/5 custom images\n",
    "\n",
    "### Textual inevrsion :\n",
    " - it injects a custom subject into the model wiht just few examples, all traning is done only in the embedding neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YC66HByIOF-w"
   },
   "source": [
    "# Dreambooth\n",
    "- method to fine tune text to image models like stable diffusion.\n",
    "- It solves two problems when inseting the object into the model : overfitting(as dataset is too small) and langauge drift.\n",
    "- Using the rare words\n",
    "- Preservation of the class: to preserve the meaningo of the class , the model is adjusted ins uch a way that the subject is injected while the generation of the class image is preserved.\n",
    "## It requires 3  cthings for training:\n",
    "- 1 Unique Identifier\n",
    "- 2 Class name\n",
    "- 3 Images of the subject to be inserted\n",
    "## In our Implementation we are going to using the face of a person , class is person\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1758318081953,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "vzY_ZPndJQ42"
   },
   "outputs": [],
   "source": [
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1758318101691,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "5XHFzCROQgbF"
   },
   "outputs": [],
   "source": [
    "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15138,
     "status": "ok",
     "timestamp": 1758318124387,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "_mQi2Z2AQrg7",
    "outputId": "a9d1fedb-2f61-4a43-d166-447671bd9d46"
   },
   "outputs": [],
   "source": [
    "! pip install -qq git+https://github.com/ShivamShrirao/diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1974,
     "status": "ok",
     "timestamp": 1758318147893,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "LkaW7HypQ3ps",
    "outputId": "b92d84d0-0ef5-4afd-f963-c368fea1d763"
   },
   "outputs": [],
   "source": [
    "! pip install -q -U -pre triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13686,
     "status": "ok",
     "timestamp": 1758318165041,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "bepPOeMJQ_1U",
    "outputId": "ebf9c2c5-e2e4-4c20-9f6d-cbdc04e9f2a8"
   },
   "outputs": [],
   "source": [
    "! pip install -q accelerate transformers ftfy bitsandbytes gradio natsort safetensors xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758318203698,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "lzGwJzDYRJlK"
   },
   "outputs": [],
   "source": [
    "model_sd = \"runyaml/stable-diffusion-v1-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1758318599602,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "Z_KlssDkRgw8"
   },
   "outputs": [],
   "source": [
    "output_dir =\"/content/stable_diffusion_weights/zwx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1758318602335,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "vFdOylkKRt2u"
   },
   "outputs": [],
   "source": [
    "! mkdir -p $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1758318637065,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "Pe5zJYfKR6gx",
    "outputId": "fdc0e135-7435-4ec4-f1fc-6059d013d636"
   },
   "outputs": [],
   "source": [
    "! unzip /content/stable_diffusion_weights/zwx/dataset_dave-20250917T185257Z-1-001.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgoKOHTDT_U5"
   },
   "source": [
    "# Training :\n",
    "1) unique Identifier\n",
    "2) Class Name\n",
    "3) Images\n",
    "- class prompt: photo of [class name]\n",
    "- instance prompt will be a zwx person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hngBaA3tW7Wo"
   },
   "source": [
    "# Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758318640188,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "ZPHsXZtCT-Nc"
   },
   "outputs": [],
   "source": [
    "concepts_list = [{\n",
    "    \"instance_prompt\":     \"photo of zwx\",\n",
    "    \"class_prompt\":        \"photo of a person\",\n",
    "    \"instance_data_dir\":   \"/content/dataset_dave\",\n",
    "    \"class_data_dir\":      \"/content/dataset_dave\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758318641418,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "qwwVBqxzVNbJ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1758318642633,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "-Ro0XZvVVUVG"
   },
   "outputs": [],
   "source": [
    "for c in concepts_list:\n",
    "  os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
    "  os.makedirs(c[\"class_data_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1758318644283,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "FeAObmd5Vc3G"
   },
   "outputs": [],
   "source": [
    "with open(\"concepts_list.json\", \"w\") as f:\n",
    "    json.dump(concepts_list, f, indent=4)\n",
    "    # created he json file to provide the it to the model for the training\n",
    "    \"\"\"provide the concepts_list.json file because it acts as a configuration file for the training script (train_dreambooth.py). This JSON file contains crucial information that the script needs to know, such as:\n",
    "\n",
    "instance_prompt: The unique phrase associated with the subject you want to train (e.g., \"photo of zwx\").\n",
    "class_prompt: The general category of the subject (e.g., \"photo of a person\").\n",
    "instance_data_dir: The directory containing the images of your specific subject.\n",
    "class_data_dir: The directory containing images of the general class.\n",
    "The training script will read this JSON file to understand what to train and where to find the necessary data. It then uses this information to fine-tune the Stable Diffusion model.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12866,
     "status": "ok",
     "timestamp": 1758318659249,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "_VKBIPobl1uu",
    "outputId": "6a6d82a9-74a2-44f6-f49d-ced79ecac807"
   },
   "outputs": [],
   "source": [
    "# in your notebook / shell\n",
    "! pip install --upgrade pip\n",
    "! pip install --upgrade diffusers huggingface_hub\n",
    "# then restart the Python kernel / runtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCFcFXGoW32g"
   },
   "source": [
    "# Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1758318672204,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "ZnvcybHLWCno",
    "outputId": "0fad6d42-8bfa-4db7-a8b9-2ca82b57ce61"
   },
   "outputs": [],
   "source": [
    "num_images = 10\n",
    "num_class_images = num_images*12\n",
    "max_num_steps =num_images*80 # default equation\n",
    "learning_rate = 1e-6\n",
    "lr_warmup_steps = int(max_num_steps/10)\n",
    "print(num_images, num_class_images, max_num_steps, learning_rate, lr_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26475,
     "status": "ok",
     "timestamp": 1758318709501,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "dO2YCm8qmVnR",
    "outputId": "81c0f272-81ab-43ce-be51-3464ab1b41d4"
   },
   "outputs": [],
   "source": [
    "!python3 -c \"from diffusers import StableDiffusionPipeline; print('diffusers import OK')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5QUWzE8mVFy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14681,
     "status": "ok",
     "timestamp": 1758318752514,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "WKYwORIuWtMR",
    "outputId": "f86f1e12-52a6-4ffe-fe27-4b52d3f813b4"
   },
   "outputs": [],
   "source": [
    "\n",
    "! python3 train_dreambooth.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1375375,
     "status": "ok",
     "timestamp": 1758320147537,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "019c4715",
    "outputId": "056b4e17-64b3-46ac-fa89-27c00d48b596"
   },
   "outputs": [],
   "source": [
    "! python3 train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\"\\\n",
    "  --output_dir=$output_dir \\\n",
    "  --concepts_list=\"concepts_list.json\" \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --instance_data_dir=\"/content/dataset_dave\" \\\n",
    "  --class_data_dir=\"/content/dataset_dave\" \\\n",
    "  --instance_prompt=\"photo of zwx person\" \\\n",
    "  --class_prompt=\"photo of a person\" \\\n",
    "  --num_class_images=$num_class_images \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --sample_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=$learning_rate \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=$lr_warmup_steps \\\n",
    "  --max_train_steps=$max_num_steps \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --gradient_checkpointing \\\n",
    "  --use_8bit_adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EjiSls4uBXh"
   },
   "source": [
    "Sure! Let’s go **step by step** through this output so you understand what’s happening during your DreamBooth training run. I’ll break it into sections and explain what each part means, why it happens, and whether it’s a problem.\n",
    "\n",
    "---\n",
    "\n",
    "## **1️⃣ CUDA / TensorFlow warnings**\n",
    "\n",
    "```\n",
    "2025-09-18 07:04:14.372762: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
    "...\n",
    "W0000 00:00:1758179054.414767    7148 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
    "2025-09-18 07:04:14.419370: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions...\n",
    "```\n",
    "\n",
    "**What it is:**\n",
    "\n",
    "* TensorFlow is trying to initialize CUDA libraries: **cuFFT, cuDNN, cuBLAS**.\n",
    "* It detects that these libraries are already loaded (probably by PyTorch or XLA).\n",
    "\n",
    "**Impact:**\n",
    "\n",
    "* These are **warnings, not errors**.\n",
    "* Training continues normally in PyTorch (your DreamBooth uses PyTorch, not TensorFlow).\n",
    "* You don’t need to worry unless you actually see CUDA errors stopping training.\n",
    "\n",
    "---\n",
    "\n",
    "## **2️⃣ Fetching model files**\n",
    "\n",
    "```\n",
    "config.json: 100% 547/547 [00:00<00:00, 3.79MB/s]\n",
    "vae/diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 275MB/s]\n",
    "...\n",
    "text_encoder/model.safetensors: 100% 492M/492M [00:03<00:00, 125MB/s]\n",
    "unet/diffusion_pytorch_model.safetensors: 100% 3.44G/3.44G [00:23<00:00, 146MB/s]\n",
    "```\n",
    "\n",
    "**What it is:**\n",
    "\n",
    "* DreamBooth downloads the **pretrained Stable Diffusion components** from Hugging Face:\n",
    "\n",
    "  1. **VAE** (`vae/diffusion_pytorch_model.safetensors`) – encoder/decoder for images.\n",
    "  2. **UNet** (`unet/diffusion_pytorch_model.safetensors`) – the main generative model.\n",
    "  3. **Text encoder** (`text_encoder/model.safetensors`) – converts prompts into embeddings.\n",
    "  4. Configs, tokenizer, scheduler, etc.\n",
    "\n",
    "**Why important:**\n",
    "\n",
    "* These are the **base models** DreamBooth fine-tunes on your images.\n",
    "* Without them, the training cannot start.\n",
    "\n",
    "**Observation:**\n",
    "\n",
    "* You can see the download progress for large files (`3.44G` UNet) and smaller files.\n",
    "* Slow speeds at first are normal if the internet is slow; caching helps for next runs.\n",
    "\n",
    "---\n",
    "\n",
    "## **3️⃣ Safety checker warning**\n",
    "\n",
    "```\n",
    "You have disabled the safety checker for ... by passing `safety_checker=None`...\n",
    "```\n",
    "\n",
    "**What it is:**\n",
    "\n",
    "* Diffusers has a **safety filter** to block NSFW content.\n",
    "* You disabled it (`safety_checker=None`) to avoid warnings during training or generation.\n",
    "\n",
    "**Impact:**\n",
    "\n",
    "* No effect on DreamBooth training itself.\n",
    "* Only matters if you plan to deploy the model publicly; make sure you comply with license rules.\n",
    "\n",
    "---\n",
    "\n",
    "## **4️⃣ Generating class images**\n",
    "\n",
    "```\n",
    "Generating class images: 100% 110/110 [06:14<00:00, 3.40s/it]\n",
    "```\n",
    "\n",
    "**What it is:**\n",
    "\n",
    "* DreamBooth generates **class images** (generic “person”) to prevent overfitting.\n",
    "* `--with_prior_preservation` + `--prior_loss_weight` uses these images to regularize training.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* You have 10 instance images of “zwx”.\n",
    "* DreamBooth generates 110 generic person images (class images) to help the model **not forget general human features**.\n",
    "\n",
    "**Impact if skipped:**\n",
    "\n",
    "* Model may **overfit** to the small instance dataset → outputs will only look like your few images, losing generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## **5️⃣ Caching latents**\n",
    "\n",
    "```\n",
    "Caching latents: 100% 120/120 [00:23<00:00, 5.01it/s]\n",
    "```\n",
    "\n",
    "**What it is:**\n",
    "\n",
    "* Converts images into **latent space** representations before training.\n",
    "* Latents are smaller tensors that UNet operates on instead of full images.\n",
    "\n",
    "**Why important:**\n",
    "\n",
    "* Speeds up training: model doesn’t recompute encoder every step.\n",
    "* Saves VRAM and computation.\n",
    "\n",
    "---\n",
    "\n",
    "## **6️⃣ Training steps**\n",
    "\n",
    "```\n",
    "Steps: 100% 800/800 [12:34<00:00, 1.13it/s, loss=0.276, lr=1e-6]\n",
    "```\n",
    "\n",
    "**What it is:**\n",
    "\n",
    "* Model has finished **800 training steps**.\n",
    "* `loss=0.276` → shows how well the model is fitting the instance+class images.\n",
    "* `lr=1e-6` → current learning rate (your constant scheduler).\n",
    "\n",
    "**Key notes:**\n",
    "\n",
    "* Loss decreasing → model is learning.\n",
    "* Training time: 12 minutes 34 seconds → reasonable for a small DreamBooth setup.\n",
    "\n",
    "---\n",
    "\n",
    "## **7️⃣ Pipeline loaded**\n",
    "\n",
    "```\n",
    "Loading pipeline components...: 100% 6/6 [00:00<00:00, 88.34it/s]\n",
    "```\n",
    "\n",
    "**What it is:**\n",
    "\n",
    "* After training, the script reloads the **Stable Diffusion pipeline** with your fine-tuned weights.\n",
    "* Ready for generating images using `instance_prompt` tokens like “zwx”.\n",
    "\n",
    "---\n",
    "\n",
    "## **8️⃣ Weights saved**\n",
    "\n",
    "```\n",
    "[*] Weights saved at /content/stable_diffusion_weights/zwx/800\n",
    "```\n",
    "\n",
    "**What it is:**\n",
    "\n",
    "* Trained model weights are saved to disk.\n",
    "* `800` → corresponds to number of training steps.\n",
    "* You can now load this model for inference using your token “zwx”.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from diffusers import StableDiffusionPipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"/content/stable_diffusion_weights/zwx/800\")\n",
    "image = pipe(\"photo of zwx person wearing sunglasses\").images[0]\n",
    "image.save(\"zwx_sunglasses.png\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Summary of the log**\n",
    "\n",
    "| Stage                  | What happened                               | Importance / Effect                        |\n",
    "| ---------------------- | ------------------------------------------- | ------------------------------------------ |\n",
    "| CUDA / TF warnings     | Libraries already loaded                    | Harmless, training continues               |\n",
    "| Model files fetched    | VAE, UNet, text encoder, configs            | Needed for fine-tuning                     |\n",
    "| Safety checker         | Disabled                                    | No effect on training, only for deployment |\n",
    "| Class images generated | 110 generic images                          | Prevents overfitting (prior preservation)  |\n",
    "| Latents cached         | 120 latents                                 | Speeds up training                         |\n",
    "| Training               | 800 steps, loss 0.276                       | Model learned the subject                  |\n",
    "| Pipeline loaded        | Ready for inference                         | Can generate images with new token         |\n",
    "| Weights saved          | `/content/stable_diffusion_weights/zwx/800` | Fine-tuned model stored for future use     |\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can **draw a full visual flow** showing how **instance images, class images, latents, UNet, text encoder, and gradient updates** all work together during DreamBooth training — it makes this entire log much easier to understand.\n",
    "\n",
    "Do you want me to make that diagram?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUDjy5T3rlOk"
   },
   "source": [
    "Nice — I’ll walk through every flag you used, what it does, why it matters, recommended values, and any gotchas / interactions to watch for. I’ll also give a copy-paste-ready example (with `concepts_list.json` sample) at the end so you can run it safely.\n",
    "\n",
    "# What each argument means (line-by-line)\n",
    "\n",
    "`--pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\"`\n",
    "\n",
    "* **What:** The base model you fine-tune from. Can be a Hugging Face model id (hub) or a local folder with a pretrained Stable Diffusion model.\n",
    "* **Why it matters:** All weights (UNet, VAE, text encoder) are loaded from here. If it’s a gated model you must be authenticated (`huggingface-cli login`) or pass a token.\n",
    "* **Tip:** Use `runwayml/stable-diffusion-v1-5` (or a compatible SD checkpoint). If you use a local path, set it to the folder containing model subfolders (unet, vae, text\\_encoder, etc.).\n",
    "\n",
    "`--output_dir=$output_dir`\n",
    "\n",
    "* **What:** Where trained checkpoints / final pipeline are saved.\n",
    "* **Why:** The script writes `output_dir/<step>/` (and final save at end). If you want intermediate checkpoints, tune `--save_interval`.\n",
    "* **Tip:** Use a path on disk with enough free space.\n",
    "\n",
    "`--concepts_list=\"concepts_list.json\"`\n",
    "\n",
    "* **What:** JSON file describing one or more concepts to train. If present, the script loads this list and **ignores** the single `--instance_prompt`/`--instance_data_dir` flags.\n",
    "* **Format:** list of objects with keys: `instance_prompt`, `class_prompt`, `instance_data_dir`, `class_data_dir`. (Example below.)\n",
    "* **When to use:** Use when you want to train multiple concepts in one run or supply differing prompts/dirs per concept.\n",
    "\n",
    "`--with_prior_preservation --prior_loss_weight=1.0`\n",
    "\n",
    "* **What:** Enables DreamBooth’s *prior preservation* technique. It adds an extra loss term that keeps the model from “collapsing” widely to reproduce only your few instance images. `prior_loss_weight` scales that extra loss.\n",
    "* **Why:** Prevents overfitting and keeps the model’s notion of the general class (e.g., “a person”) while learning the specific subject.\n",
    "* **Tip:** `prior_loss_weight=1.0` is a common default. If you see over-regularization (subject not learned) lower it; if you see overfitting/inversion artifacts increase it.\n",
    "\n",
    "`--instance_data_dir=\"/content/dataset_dave\"`\n",
    "\n",
    "* **What:** Folder with your instance (subject) images to teach the model (the photos of the person/object you want to learn).\n",
    "* **Tip:** 5–20 high-quality images of multiple poses/backgrounds are commonly used. More images let you use fewer steps.\n",
    "\n",
    "`--class_data_dir=\"/content/dataset_dave\"`\n",
    "\n",
    "* **What:** Folder with class images used for prior preservation (generic images for that class, e.g., many photos of other people).\n",
    "* **Important:** **Do not** use the same set for `instance_data_dir` and `class_data_dir`. If they’re identical, prior preservation will do nothing useful — you should provide *diverse* class images (many different people) so the model learns “person” while preserving uniqueness of your instance. The script *can* generate extra class images if not enough exist (it will sample from the pretrained SD pipeline).\n",
    "\n",
    "`--instance_prompt=\"photo of zwx person\"`\n",
    "\n",
    "* **What:** The prompt string used to describe your instance images during training. This should include a unique identifier token (e.g., an unusual word like `zwx`, `sks`, or a made-up token like `<yourtoken>`).\n",
    "* **Why:** The unique token ties the subject to the concept in the model’s latent space so the model can later generate images of that subject by using this token in a prompt.\n",
    "* **Tip:** Use something unlikely to collide with normal text (e.g., `photo of zwx person` or `a photo of <myname> person`). If you later want to generate `photo of zwx person in a hat`, the model will associate `zwx` with the learned subject.\n",
    "\n",
    "`--class_prompt=\"photo of a person\"`\n",
    "\n",
    "* **What:** Generic prompt used for class images. It should describe the general class, e.g., “photo of a person”.\n",
    "* **Why:** Paired with prior preservation to keep model’s general knowledge of that class.\n",
    "\n",
    "`--num_class_images=$num_class_images`\n",
    "\n",
    "* **What:** Target number of class images used for prior preservation. The script will sample/generate additional class images to reach this number if needed.\n",
    "* **Tip:** Typical values: 100–200+. The higher the better for strong prior preservation, but generates more class images and uses more disk/time.\n",
    "\n",
    "`--resolution=512`\n",
    "\n",
    "* **What:** Training image size (images are resized/cropped to this). 512 is standard for many SD models.\n",
    "* **Why:** The model architecture expects a multiple of 64 typically; using the same resolution as the pretrained model avoids mismatch.\n",
    "* **Tip:** Use the model’s native resolution (512 or 768 depending on model). Larger resolution = more VRAM and slower training.\n",
    "\n",
    "`--train_batch_size=1`\n",
    "\n",
    "* **What:** Per-device batch size for training.\n",
    "* **Why:** Larger batch sizes speed training but need more GPU memory. DreamBooth commonly uses `1`. You can use gradient accumulation to simulate larger effective batch sizes.\n",
    "\n",
    "`--sample_batch_size=1`\n",
    "\n",
    "* **What:** Batch size used when generating class images (sampling pipeline). Keep small to avoid GPU OOM during generation.\n",
    "\n",
    "`--gradient_accumulation_steps=1`\n",
    "\n",
    "* **What:** Number of mini-batches to accumulate gradients over before stepping optimizer.\n",
    "* **Why:** Useful to simulate larger batch sizes if GPU memory is limited: `effective_batch = train_batch_size * gradient_accumulation_steps * num_gpus`.\n",
    "* **Tip:** If you want an effective batch of 4 but only fit batch size 1, set `gradient_accumulation_steps=4`.\n",
    "\n",
    "`--learning_rate=$learning_rate`\n",
    "\n",
    "* **What:** Base learning rate for optimizer. DreamBooth often uses small LRs: `1e-6` to `5e-6` (common starting point `5e-6`).\n",
    "* **Tip:** Too large → destroys pretrained weights; too small → slow/ineffective learning. Tune carefully.\n",
    "\n",
    "`--lr_scheduler=\"constant\"`\n",
    "\n",
    "* **What:** Which LR scheduler to use. `\"constant\"` keeps LR fixed (but may support warmup if implemented). Other options: `linear`, `cosine`, `polynomial`, etc.\n",
    "* **Tip:** Constant or constant\\_with\\_warmup is common for finetuning.\n",
    "\n",
    "`--lr_warmup_steps=$lr_warmup_steps`\n",
    "\n",
    "* **What:** Number of warmup steps at start where LR ramps up from 0 to `learning_rate`.\n",
    "* **Why:** Small warmup stabilizes training at start. Typical `lr_warmup_steps` is small (0–100).\n",
    "\n",
    "`--max_train_steps=$max_num_steps`\n",
    "\n",
    "* **What:** Total optimizer steps to perform. Overrides `num_train_epochs` if set.\n",
    "* **Guideline:** DreamBooth common ranges:\n",
    "\n",
    "  * If you have 10–20 images: **800–2000** steps often reasonable.\n",
    "  * If you have many images, scale steps accordingly.\n",
    "* **Tip:** Monitor outputs; you can stop early if results are good, or increase if underfitting.\n",
    "\n",
    "`--mixed_precision=\"fp16\"`\n",
    "\n",
    "* **What:** Use half-precision (FP16) training for memory and speed. Alternatives: `bf16` (if hardware supports) or `no`.\n",
    "* **Why:** FP16 reduces VRAM usage and speeds up many GPUs.\n",
    "* **Caveats:** BF16 is safer numerically but requires GPU and PyTorch support. FP16 can cause instabilities on some setups — use gradient scaling/AMP (handled by `accelerate`). If you see NaNs, try `no` or `bf16` (if supported).\n",
    "\n",
    "`--gradient_checkpointing`\n",
    "\n",
    "* **What:** Activates gradient checkpointing to reduce memory use by recomputing activations during backward pass.\n",
    "* **Tradeoff:** Uses less memory but increases compute time (\\~10–30% slower).\n",
    "\n",
    "`--use_8bit_adam`\n",
    "\n",
    "* **What:** Use 8-bit Adam optimizer (from `bitsandbytes`) which keeps optimizer state in 8-bit to drastically reduce memory used by optimizer states.\n",
    "* **Requires:** `pip install bitsandbytes` and proper CUDA / driver compatibility.\n",
    "* **Tip:** Combine `use_8bit_adam` + `gradient_checkpointing` + `fp16` to fit DreamBooth on 12–16GB GPUs.\n",
    "\n",
    "---\n",
    "\n",
    "# Extra script behavior & important interactions\n",
    "\n",
    "* **`concepts_list.json` vs CLI single flags:** If `--concepts_list` is provided, the script **loads** that and ignores the single `--instance_prompt`/`--instance_data_dir` you passed. Use one method, not both, unless your JSON intentionally depends on CLI variables.\n",
    "* **Caching latents:** By default the script caches latents (`vae.encode(...)`) to speed training and save VAE memory later. If you set `--not_cache_latents`, training will compute latents on the fly (less disk & memory to store caches, but slower).\n",
    "* **Saving:** The default `save_interval` in the script is large (10,000). If your `max_train_steps` is smaller, you’ll still get a final save at the end (the script calls `save_weights` at completion). If you want intermediate checkpoints, set `--save_interval` smaller.\n",
    "* **Class images generation:** If `class_data_dir` doesn’t have enough images, the script uses the StableDiffusion pipeline to generate more class images (using `sample_batch_size` and `save_infer_steps`). This requires the pipeline to be loaded (and that uses VRAM).\n",
    "* **Unique identifier token in `instance_prompt`:** For classic DreamBooth you usually include an identifier token (e.g., `a photo of <VAN_GOGH_STYLE> dog`), but the script as written doesn’t automatically add new tokens to the tokenizer / text encoder vocabulary — so use an uncommon short token (like `zwx` or `sks`) to avoid collisions. If you want to actually add a new token embedding, more advanced token insertion/training is required (not in this script).\n",
    "* **If you use gated models (like official Stable Diffusion weights):** you must be authenticated to Hugging Face (`huggingface-cli login`) or pass `--hub_token`/environment var.\n",
    "\n",
    "---\n",
    "\n",
    "# Example `concepts_list.json`\n",
    "\n",
    "If you want to train multiple concepts or prefer JSON config, here’s an example file:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"instance_prompt\": \"photo of zwx person\",\n",
    "    \"class_prompt\": \"photo of a person\",\n",
    "    \"instance_data_dir\": \"/content/dataset_dave\",\n",
    "    \"class_data_dir\": \"/content/class_images_person\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "Place that as `concepts_list.json` and then call the script with `--concepts_list=\"concepts_list.json\"`. The script will iterate concepts.\n",
    "\n",
    "---\n",
    "\n",
    "# Recommended starting hyperparameters (common DreamBooth recipe)\n",
    "\n",
    "* `train_batch_size=1`\n",
    "* `gradient_accumulation_steps=1` (increase if you want larger effective batch)\n",
    "* `learning_rate=5e-6` (try 1e-6 → 5e-6)\n",
    "* `max_train_steps=800` → increase to 1000–2000 for better fidelity with very few images\n",
    "* `num_class_images=100` (or 200 if you want stronger prior)\n",
    "* `mixed_precision=fp16` (if you have an NVIDIA GPU that supports it)\n",
    "* `gradient_checkpointing` = ON (saves VRAM)\n",
    "* `use_8bit_adam` = ON if you installed `bitsandbytes` and want to save optimizer memory\n",
    "\n",
    "---\n",
    "\n",
    "# Example full command (copy-paste, adjust variables)\n",
    "\n",
    "```bash\n",
    "export output_dir=\"./dreambooth_dave\"\n",
    "export num_class_images=200\n",
    "export learning_rate=5e-6\n",
    "export lr_warmup_steps=0\n",
    "export max_num_steps=1000\n",
    "export model_sd=\"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "python3 train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=\"$model_sd\" \\\n",
    "  --output_dir=\"$output_dir\" \\\n",
    "  --concepts_list=\"concepts_list.json\" \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --instance_data_dir=\"/content/dataset_dave\" \\\n",
    "  --class_data_dir=\"/content/class_images_person\" \\\n",
    "  --instance_prompt=\"photo of zwx person\" \\\n",
    "  --class_prompt=\"photo of a person\" \\\n",
    "  --num_class_images=$num_class_images \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --sample_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=$learning_rate \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=$lr_warmup_steps \\\n",
    "  --max_train_steps=$max_num_steps \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --gradient_checkpointing \\\n",
    "  --use_8bit_adam\n",
    "```\n",
    "\n",
    "**Note:** make sure you are logged in to Hugging Face if the model is gated:\n",
    "\n",
    "```bash\n",
    "huggingface-cli login\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Final checklist before you run\n",
    "\n",
    "* `huggingface-cli login` (if model gated)\n",
    "* `concepts_list.json` exists and points to the correct folders, or remove `--concepts_list` and pass `--instance_data_dir`/`--instance_prompt` directly.\n",
    "* `class_data_dir` contains diverse class images (not identical to instance images).\n",
    "* `bitsandbytes` installed if using `--use_8bit_adam`.\n",
    "* GPU memory: if you run out of memory, enable `gradient_checkpointing`, `use_8bit_adam`, reduce `resolution`, or reduce `train_batch_size` / use `gradient_accumulation_steps`.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can:\n",
    "\n",
    "* Generate a ready `concepts_list.json` from your dataset paths, or\n",
    "* Suggest specific `max_train_steps`/`learning_rate` tuned to the number of instance images you have (tell me how many images you have).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7Mj8KXns16U"
   },
   "source": [
    "Absolutely! Let’s break each of these options **thoroughly**, with examples, why they matter, and what happens if you skip them. I’ll also explain the **practical impact** on training DreamBooth models.\n",
    "\n",
    "---\n",
    "\n",
    "## **1️⃣ `--concepts_list=\"concepts_list.json\"`**\n",
    "\n",
    "**What it does:**\n",
    "\n",
    "* Allows you to train **one or more concepts** (subjects) in a single run.\n",
    "* Each concept can have its own images and prompts.\n",
    "* If this is provided, the script **ignores** single flags like `--instance_prompt` and `--instance_data_dir`.\n",
    "\n",
    "**Format of the JSON:**\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"instance_prompt\": \"photo of zwx person\",\n",
    "    \"class_prompt\": \"photo of a person\",\n",
    "    \"instance_data_dir\": \"/content/dataset_dave\",\n",
    "    \"class_data_dir\": \"/content/class_images_person\"\n",
    "  },\n",
    "  {\n",
    "    \"instance_prompt\": \"photo of catxyz\",\n",
    "    \"class_prompt\": \"photo of a cat\",\n",
    "    \"instance_data_dir\": \"/content/dataset_cat\",\n",
    "    \"class_data_dir\": \"/content/class_images_cat\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* You want to teach the model **two subjects**: a person (`zwx`) and a cat (`catxyz`).\n",
    "* You create the above JSON. Now one command trains both concepts in one run.\n",
    "\n",
    "**Why important:**\n",
    "\n",
    "* Simplifies multi-concept training.\n",
    "* Ensures proper separation of prompts, datasets, and prior images per concept.\n",
    "\n",
    "**If missing:**\n",
    "\n",
    "* You can still train one concept using `--instance_prompt` and `--instance_data_dir`, but **multi-concept training** would require multiple runs manually.\n",
    "\n",
    "---\n",
    "\n",
    "## **2️⃣ `--instance_prompt=\"photo of zwx person\"`**\n",
    "\n",
    "**What it does:**\n",
    "\n",
    "* This is the **text prompt** describing your instance images during training.\n",
    "* The model associates the unique token `zwx` with your subject.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* You have 10 images of a person named Dave.\n",
    "* You set `--instance_prompt=\"photo of zwx person\"`.\n",
    "* After training, you can generate images like:\n",
    "\n",
    "  ```text\n",
    "  \"photo of zwx person wearing a hat\"\n",
    "  ```\n",
    "\n",
    "  and the model knows “zwx” = Dave.\n",
    "\n",
    "**Why important:**\n",
    "\n",
    "* Without this, the model cannot know which part of the image is your subject.\n",
    "* This unique token allows the model to differentiate your instance from other generic subjects.\n",
    "\n",
    "**If missing:**\n",
    "\n",
    "* The model may just learn generic “person” features, losing specificity.\n",
    "* Outputs won’t look like the person/object you want.\n",
    "\n",
    "---\n",
    "\n",
    "## **3️⃣ `--gradient_accumulation_steps=1`**\n",
    "\n",
    "**What it does:**\n",
    "\n",
    "* Accumulates gradients over multiple mini-batches **before updating the model weights**.\n",
    "* Simulates a **larger batch size** without needing more GPU memory.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "```\n",
    "effective_batch_size = train_batch_size * gradient_accumulation_steps * num_gpus\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* `train_batch_size=1`\n",
    "* `gradient_accumulation_steps=4`\n",
    "* Effective batch size = 4\n",
    "* The optimizer updates only after 4 images have been processed.\n",
    "\n",
    "**Why important:**\n",
    "\n",
    "* Helps train on GPUs with **limited memory** while maintaining stable gradient updates.\n",
    "* Larger batch sizes = smoother training, fewer oscillations.\n",
    "\n",
    "**If missing:**\n",
    "\n",
    "* Batch size = 1 in the above example.\n",
    "* Could cause unstable training if your dataset is tiny or the learning rate is too high.\n",
    "\n",
    "---\n",
    "\n",
    "## **4️⃣ `--lr_scheduler=\"constant\"`**\n",
    "\n",
    "**What it does:**\n",
    "\n",
    "* Controls how learning rate (LR) changes during training.\n",
    "* `\"constant\"` keeps the learning rate **the same** for all steps (may still support warmup).\n",
    "\n",
    "**Other options:**\n",
    "\n",
    "* `\"linear\"`: LR decreases linearly to 0.\n",
    "* `\"cosine\"`: LR follows a cosine decay curve.\n",
    "* `\"polynomial\"`: LR decreases polynomially.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* `learning_rate=5e-6`\n",
    "* `lr_scheduler=constant` → keeps 5e-6 throughout training.\n",
    "* Simple, stable for DreamBooth fine-tuning.\n",
    "\n",
    "**Why important:**\n",
    "\n",
    "* LR schedule helps the model converge better.\n",
    "* Constant LR is good for small fine-tuning (few images).\n",
    "\n",
    "**If missing:**\n",
    "\n",
    "* Default scheduler may be used (depends on script).\n",
    "* Could cause slow convergence or overfitting if LR not suited.\n",
    "\n",
    "---\n",
    "\n",
    "## **5️⃣ `--mixed_precision=\"fp16\"`**\n",
    "\n",
    "**What it does:**\n",
    "\n",
    "* Uses **half-precision floats (16-bit)** instead of full 32-bit floats for training.\n",
    "* Saves GPU memory and speeds up training.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Standard FP32 might use **16 GB VRAM** for DreamBooth.\n",
    "* FP16 reduces usage to **\\~8–10 GB VRAM** on the same model.\n",
    "\n",
    "**Why important:**\n",
    "\n",
    "* Allows training large models (SD) on GPUs with limited VRAM.\n",
    "* Speeds up computations because FP16 math is faster on modern GPUs (Tensor Cores).\n",
    "\n",
    "**If missing:**\n",
    "\n",
    "* Training will use FP32 → higher memory usage.\n",
    "* Might not fit in your GPU if VRAM is small.\n",
    "\n",
    "**Caveats:**\n",
    "\n",
    "* FP16 can cause **NaN / instability** if gradients explode.\n",
    "* Alternative: `bf16` (if GPU supports) → safer numerically.\n",
    "\n",
    "---\n",
    "\n",
    "## **6️⃣ `--gradient_checkpointing`**\n",
    "\n",
    "**What it does:**\n",
    "\n",
    "* Reduces GPU memory usage by **recomputing activations during backward pass** instead of storing them.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Without checkpointing: model stores all activations → uses **16 GB VRAM**.\n",
    "* With checkpointing: recomputes activations → uses **\\~8 GB VRAM** but training is slightly slower.\n",
    "\n",
    "**Why important:**\n",
    "\n",
    "* Lets you train **larger batch sizes** or **higher resolutions** on limited VRAM GPUs.\n",
    "\n",
    "**If missing:**\n",
    "\n",
    "* Uses more memory → may **OOM** (Out of Memory) errors.\n",
    "* Training faster (because recomputation isn’t needed).\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary Table\n",
    "\n",
    "| Argument                      | What it does                                  | Why important                     | If missing                                    |\n",
    "| ----------------------------- | --------------------------------------------- | --------------------------------- | --------------------------------------------- |\n",
    "| `concepts_list`               | JSON list of concepts                         | Multi-concept training in one run | Must train each concept separately            |\n",
    "| `instance_prompt`             | Text description of your instance             | Teaches model to identify subject | Model learns generic features only            |\n",
    "| `gradient_accumulation_steps` | Accumulate gradients for effective batch size | Stability on small GPUs           | Smaller batch, unstable gradients             |\n",
    "| `lr_scheduler`                | Controls learning rate schedule               | Smooth convergence                | Could overfit / underfit                      |\n",
    "| `mixed_precision`             | Uses FP16 training                            | Reduces VRAM, speeds up           | Might OOM, slower                             |\n",
    "| `gradient_checkpointing`      | Recomputes activations to save memory         | Enables training on low VRAM      | May OOM if VRAM insufficient, faster training |\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Example**\n",
    "\n",
    "Assume:\n",
    "\n",
    "* 10 instance images of “Dave”\n",
    "* 100 class images (generic “person”)\n",
    "* GPU: 12GB\n",
    "\n",
    "```bash\n",
    "python3 train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n",
    "  --output_dir=\"./output_dave\" \\\n",
    "  --concepts_list=\"concepts_list.json\" \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --instance_prompt=\"photo of zwx person\" \\\n",
    "  --class_prompt=\"photo of a person\" \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=2 \\\n",
    "  --learning_rate=5e-6 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --max_train_steps=800 \\\n",
    "  --resolution=512 \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --gradient_checkpointing \\\n",
    "  --use_8bit_adam\n",
    "```\n",
    "\n",
    "**Explanation of resource savings:**\n",
    "\n",
    "* `train_batch_size=1` + `gradient_accumulation_steps=2` → effective batch = 2\n",
    "* `fp16` + `gradient_checkpointing` → fits SD training in 12GB VRAM\n",
    "* Prior preservation (`with_prior_preservation`) → avoids overfitting to 10 images\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can **draw a diagram showing how `instance_prompt`, `class_prompt`, concepts list, gradient accumulation, FP16, and checkpointing interact** during DreamBooth training — it makes the flow super clear visually.\n",
    "\n",
    "Do you want me to do that?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1758323366550,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "mICUS6Dis0lQ"
   },
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "import os\n",
    "# Use glob to find directories that look like step numbers, then sort and take the last one.\n",
    "# This assumes your training script saves checkpoints in directories named by step number.\n",
    "# Alternatively, if you know the exact step number, you can specify the directory directly.\n",
    "# weights_dir = natsorted(glob(os.path.join(output_dir, \"[0-9]*\")))[-1]\n",
    "\n",
    "# Explicitly setting the weights directory as requested by the user.\n",
    "weights_dir = \"/content/stable_diffusion_weights/zwx/800\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1758323553991,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "c4sf4SV_uO-k",
    "outputId": "38a4fcb7-a9d5-476a-afd2-2665a46413ff"
   },
   "outputs": [],
   "source": [
    "print(\"weight directory:\" + weights_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJAxxchCwE6o"
   },
   "source": [
    "# convert the weights into checkpoints :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 208330,
     "status": "ok",
     "timestamp": 1758323849082,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "3xfhGkdDwH7U",
    "outputId": "cf3bb22a-e9a9-4a3d-acb5-f515c6223a2c"
   },
   "outputs": [],
   "source": [
    "ckpt_path = os.path.join(weights_dir, \"model.ckpt\")\n",
    "half_arg = \"--half\" # fp 16\n",
    "!python /content/convert_diffusers_to_original_stable_diffusion.py --model_path $weights_dir --checkpoint_path $ckpt_path $half_arg\n",
    "print(f\"  ckpt saved at {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1758323849896,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "ITMJa3LxyQUj",
    "outputId": "64397496-9b57-44f1-bca3-4718a04bc789"
   },
   "outputs": [],
   "source": [
    "weights_folder = output_dir\n",
    "# Filter out items that are not directories or cannot be converted to integers before sorting\n",
    "folders = sorted([f for f in os.listdir(weights_folder) if os.path.isdir(os.path.join(weights_folder, f)) and f.isdigit() and f !=\"0\"], key =lambda x: int(x))\n",
    "imgs_test =[]\n",
    "\n",
    "for folder in folders:\n",
    "  folder_path = os.path.join(weights_folder, folder)\n",
    "  image_folder = os.path.join(folder_path, \"/content/dataset_dave\") # Corrected path to sample images\n",
    "\n",
    "  # Check if the samples directory exists and is a directory\n",
    "  if os.path.isdir(image_folder):\n",
    "      images = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))] # Filter for files\n",
    "      images.sort() # Sort images within the folder for consistent order\n",
    "\n",
    "      # Take the first few images if more than needed, or all if less than or equal to 3\n",
    "      images_to_display = images[:3] # Take up to the first 3 images\n",
    "\n",
    "      for i in images_to_display:\n",
    "          img_path = os.path.join(image_folder, i)\n",
    "          try:\n",
    "              img = Image.open(img_path)\n",
    "              imgs_test.append(img)\n",
    "          except Exception as e:\n",
    "              print(f\"Error opening image {img_path}: {e}\")\n",
    "  else:\n",
    "      print(f\"Samples directory not found in {folder_path}\")\n",
    "\n",
    "\n",
    "# Display the images in a grid (assuming we have enough images for a 1x3 grid)\n",
    "if len(imgs_test) >= 3:\n",
    "    display(grid_img(imgs_test[:3], row=1, cols=3, scale=1))\n",
    "elif len(imgs_test) > 0:\n",
    "    print(f\"Only {len(imgs_test)} sample images found. Cannot create a 1x3 grid.\")\n",
    "else:\n",
    "    print(\"No sample images found in the specified directories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXwF-8EQLWol"
   },
   "source": [
    "# Inferencing;(Tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1758323443789,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "4SViBdlJLZ-l"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1758323444769,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "rBPMIQDsL0XH",
    "outputId": "be0a18ce-4938-4ba3-8527-0b972f0a0383"
   },
   "outputs": [],
   "source": [
    "model_path = weights_dir\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "7737cd796c5f4e0b80831635e9d9367d",
      "507509bdd93545b6ad6e9f5f1addaf6d",
      "24c19d13f61c4c8580444110f9b52de0",
      "6cb30c2959664fd79dc989acc7316fb8",
      "a2e83168b64f490f909f108d4dd9e160",
      "9714d765f0534db89669473d3f3f4cb7",
      "9d189b3464a3492881292b1f1ac0964d",
      "6784041e666749098a514d9966d0103d",
      "332ccd44403c46dbbb7b37b5ed20496d",
      "88cf6b966a334b229edbe73c16a3d4c6",
      "771e921c0c9f4fdb8b316dd12ca1e4c5"
     ]
    },
    "executionInfo": {
     "elapsed": 17733,
     "status": "ok",
     "timestamp": 1758323465052,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "GiHZ4dEZL_sv",
    "outputId": "7276ed5b-4cb6-4e20-b4bf-9e5b3e060bb5"
   },
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "seed = 777\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6J1H4UkNKpn"
   },
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\"):\n",
    "StableDiffusionPipeline.from_pretrained(model_path, ...): This is the core function call from the diffusers library that loads a pre-trained (or in this case, fine-tuned) Stable Diffusion model pipeline.\n",
    "model_path: This variable (which you defined earlier as /content/stable_diffusion_weights/zwx/800) specifies the directory where your fine-tuned model weights are saved. The from_pretrained method knows how to load the various components (UNet, VAE, text encoder, etc.) from this directory.\n",
    "torch_dtype=torch.float16: This argument tells PyTorch to load the model weights in half-precision (16-bit floating-point format). This is a common optimization for deep learning models on GPUs, as it significantly reduces memory usage and can speed up computation.\n",
    ".to(\"cuda\"): This part moves the entire pipeline model to your GPU (CUDA device). This is necessary to perform inference efficiently using the GPU's parallel processing capabilities.\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config):\n",
    "pipe.scheduler: Accesses the default scheduler that came with the loaded pipeline.\n",
    "pipe.scheduler.config: Retrieves the configuration settings of the default scheduler.\n",
    "DDIMScheduler.from_config(...): Creates a new DDIMScheduler instance using the configuration of the original scheduler. This replaces the pipeline's default scheduler with a DDIM scheduler, which is another popular and often faster sampling method for generating images with Stable Diffusion.\n",
    "In essence, this cell loads your fine-tuned model weights into a Stable Diffusion pipeline object, optimizes it for GPU usage with half-precision, and sets up the DDIM scheduler for the image generation process. After this cell runs successfully, the pipe object is ready to be used for generating images based on text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "0b7f8e9beccd459c9afce75ee743dc51",
      "663ed3e463e943a58fefd0c74ad8a653",
      "c204d84019614db1a3445481e7b100f6",
      "958984ff87814f5cbdb633d9ebb7f2fc",
      "d3aa239cbd554e268a1276d2cc9c241a",
      "7efd9e956b664a8b91a51385820171bd",
      "ad28858f006b419a8257d0c5e93c4847",
      "733064b9899443fb828b731385d73a33",
      "33d5457a22ea4726bbf59a6c1b6b5c7d",
      "c76b15ceb1cb4867b29fbf897dbc8b16",
      "c45e3f3cc9ad47e1bc9a20d496c7d1db"
     ]
    },
    "executionInfo": {
     "elapsed": 23385,
     "status": "ok",
     "timestamp": 1758323950557,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "yvLSTo5cMuej",
    "outputId": "36343e85-3089-45a2-c65d-fc6ab97d226f"
   },
   "outputs": [],
   "source": [
    "prompt = \"face potrait of zwx in the snow, realistic , hd, vivid , sunset \"\n",
    "negative_prompt = \"bad anatomy, ugly, deformed, desfigured, distored face, poorly drawn hands, poorly drawn face , low definition, lowres, out of frae, low quality\"\n",
    "num_samples = 5\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 30\n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "seed = random.randint(0, 2147483647)\n",
    "print(f\"seed: {seed}\")\n",
    "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "with autocast(\"cuda\"):\n",
    "    images = pipe(\n",
    "        prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=num_samples,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator,\n",
    "    ).images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1QjQ8ypU65NEYQuEZmtSC3pkI1qFDJQ0s"
    },
    "executionInfo": {
     "elapsed": 913,
     "status": "ok",
     "timestamp": 1758323965269,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "Opzp6fvhOU2C",
    "outputId": "015c8900-3f71-4895-f500-7f0c24a1fe8d"
   },
   "outputs": [],
   "source": [
    "for i, image in enumerate(images):\n",
    "    display(image)\n",
    "    image.save(f\"output{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWYnG453QSuz"
   },
   "source": [
    "# Saving the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1758323849363,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "Xrrd0I3OQSH9"
   },
   "outputs": [],
   "source": [
    "!mkdir results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1758323969893,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "X8qQ7uGlQh9L",
    "outputId": "f50f8981-de4f-4593-fa7d-d47a9d0b21c2"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "if 'images' in locals() and isinstance(images, list) and len(images) > 0:\n",
    "    # Check if the items in the list are PIL Images\n",
    "    if all(isinstance(img, Image.Image) for img in images):\n",
    "        for i, image in enumerate(images):\n",
    "            image.save(f\"results/output{i}.png\")\n",
    "        print(f\"Successfully saved {len(images)} images to the 'results' directory.\")\n",
    "    else:\n",
    "        print(\"Error: The 'images' variable does not contain PIL Image objects. Please ensure the inference cell ran successfully.\")\n",
    "else:\n",
    "    print(\"Error: The 'images' variable is not available or is empty. Please run the inference cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9691,
     "status": "ok",
     "timestamp": 1758324312421,
     "user": {
      "displayName": "Omkar N Thakur (Om)",
      "userId": "12104069772117460097"
     },
     "user_tz": 240
    },
    "id": "89wR7Sl2Soxu",
    "outputId": "06942b37-9e42-4c6e-b980-ead5e5db3fa3"
   },
   "outputs": [],
   "source": [
    "! pip install nbstripout\n",
    "! nbstripout --install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ED551wQOTWUO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6W4zuLRoWvq6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNO7DlhgLv9Y+WxlRTIyg4U",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
